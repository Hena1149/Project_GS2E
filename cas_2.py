from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random
import spacy
from spacy.lang.fr.stop_words import STOP_WORDS
import streamlit as st
import re
import string
from io import BytesIO
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import pdfminer.high_level
import docx
from functools import lru_cache

# Configuration de l'application
st.set_page_config(page_title="G√©n√©rateur de Cas de test √† partir du CDC", layout="wide", page_icon="üìë")

# ----------------------------
# FONCTIONS UTILITAIRES AM√âLIOR√âES
# ----------------------------

@st.cache_resource
def load_nlp_model():
    """Charge le mod√®le spaCy pour le traitement NLP"""
    try:
        nlp = spacy.load("fr_core_news_md")
        st.success("Mod√®le NLP charg√© avec succ√®s !")
        return nlp
    except OSError:
        try:
            st.error("Mod√®le fran√ßais non trouv√©. Installation en cours...")
            import os
            os.system("python -m spacy download fr_core_news_md")
            nlp = spacy.load("fr_core_news_md")
            return nlp
        except Exception as e:
            st.error(f"√âchec du chargement : {str(e)}")
            return None

def extract_text(uploaded_file):
    """Extrait le texte depuis PDF ou DOCX"""
    try:
        file_bytes = uploaded_file.getvalue()
        
        if uploaded_file.type == "application/pdf":
            with BytesIO(file_bytes) as f:
                text = pdfminer.high_level.extract_text(f)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            with BytesIO(file_bytes) as f:
                doc = docx.Document(f)
                text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
        else:
            raise ValueError("Format non support√©")
            
        return text if text and text.strip() else None
        
    except Exception as e:
        st.error(f"Erreur d'extraction : {str(e)}")
        return None

def clean_rule_text(rule):
    """Nettoyage intelligent des r√®gles"""
    # Suppression des num√©ros et puces
    rule = re.sub(r'^[\d\s‚Ä¢\-]*', '', rule)
    # Normalisation des espaces
    rule = re.sub(r'\s+', ' ', rule).strip()
    # Correction de la ponctuation
    if rule and not rule.endswith(('.', '!', '?')):
        rule += '.'
    # Capitalisation
    return rule[0].upper() + rule[1:] if rule else rule

@lru_cache(maxsize=1000)
def is_similar_rule(rule1, rule2, threshold=0.75):
    """D√©tection de similarit√© s√©mantique entre r√®gles"""
    if 'nlp' not in st.session_state:
        st.session_state.nlp = load_nlp_model()
    doc1 = st.session_state.nlp(rule1)
    doc2 = st.session_state.nlp(rule2)
    return doc1.similarity(doc2) >= threshold

def is_potential_rule(sentence, nlp_model):
    """D√©tection avanc√©e des r√®gles potentielles"""
    if len(sentence.split()) < 6:
        return False
    
    doc = nlp_model(sentence)
    
    # Marqueurs positifs
    has_obligation = any(token.lemma_ in {'devoir', 'falloir', 'n√©cessiter'} for token in doc)
    has_validation = any(token.lemma_ in {'v√©rifier', 'contr√¥ler', 'valider'} for token in doc)
    has_condition = any(token.text.lower() in {'si', 'lorsque', 'quand'} for token in doc)
    
    # Marqueurs n√©gatifs
    is_question = any(token.tag_ == 'INTJ' for token in doc)
    is_example = any(token.text.lower() in {'exemple', 'comme'} for token in doc)
    
    return (has_obligation or has_validation or has_condition) and not (is_question or is_example)

def extract_business_rules(text, nlp_model, sensitivity=3):
    """Extraction compl√®te des r√®gles de gestion"""
    # Normalisation du texte
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'(\n\d+[.)])', r'\1 ', text)
    
    # Segmentation avanc√©e
    sentences = []
    for paragraph in text.split('\n'):
        if len(paragraph.strip()) > 10:
            if nlp_model:
                doc = nlp_model(paragraph)
                sentences.extend([sent.text for sent in doc.sents])
            else:
                sentences.extend(re.split(r'(?<=[.!?])\s+', paragraph))
    
    # Extraction avec motifs √©tendus
    patterns = [
        r'(?i)((?:doit|devra|obligatoire|requis|v√©rifier|contr√¥ler|si\b|alors\b).{8,}?[.!?])',
        r'(?i)\b(?:le syst√®me|l\'application)\b.{8,}?[.!?]',
        r'(?i)((?:lorsque|quand|d√®s que|en cas de).{8,}?(?:alors|donc|par cons√©quent).{8,}?[.!?])',
        r'(?i)(?:l\'utilisateur doit|il est n√©cessaire que).{8,}?[.!?]'
    ]
    
    rules = set()
    for pattern in patterns:
        matches = re.findall(pattern, text)
        rules.update([clean_rule_text(m) for m in matches])
    
    # Analyse syntaxique compl√©mentaire
    if nlp_model:
        for sent in sentences:
            if is_potential_rule(sent, nlp_model):
                rules.add(clean_rule_text(sent))
    
    # Filtrage adaptatif selon la sensibilit√©
    min_length = [12, 10, 8, 6, 4][min(sensitivity-1, 4)]
    rules = [r for r in rules if len(r.split()) >= min_length]
    
    # D√©doublonnage s√©mantique
    unique_rules = []
    for rule in sorted(rules, key=len, reverse=True):
        if not any(is_similar_rule(rule, existing) for existing in unique_rules):
            unique_rules.append(rule)
    
    return unique_rules[:200]  # Limite raisonnable

def generate_pdc_from_rule(rule):
    """G√©n√©ration intelligente de PDC"""
    transformations = [
        (r'(?i)doit (.+?)\.', r'V√©rifier que \1'),
        (r'(?i)il est obligatoire de (.+?)\.', r'Contr√¥ler que \1'),
        (r'(?i)le syst√®me doit (.+?)\.', r'Tester que le syst√®me \1'),
        (r'(?i)si (.+?), alors (.+?)\.', r'V√©rifier que lorsque \1, alors \2'),
        (r'(?i)l\'utilisateur peut (.+?)\.', r'Valider que l\'utilisateur peut \1')
    ]
    
    for pattern, replacement in transformations:
        if re.search(pattern, rule):
            pdc = re.sub(pattern, replacement, rule)
            return format_pdc(pdc)
    
    return f"V√©rifier que {rule.lower().rstrip('.')}."

def format_pdc(text):
    """Formattage professionnel des PDC"""
    text = re.sub(r'\s+', ' ', text).strip()
    if not text.endswith('.'):
        text += '.'
    return text[0].upper() + text[1:]

def create_test_case(pdc, index, is_manual=False):
    """Cr√©ation de cas de test bien formul√©s"""
    test_types = {
        'v√©rifier': 'Validation',
        'contr√¥ler': 'Contr√¥le',
        'tester': 'Test',
        'valider': 'V√©rification'
    }
    
    # D√©tection du type de test
    first_word = pdc.split()[0].lower()
    test_type = test_types.get(first_word, 'Test')
    
    return {
        "ID": f"CT-{index:03d}",
        "Type": test_type,
        "PDC": pdc,
        "Description": generate_test_description(pdc),
        "Pr√©conditions": "1. Environnement de test configur√©\n2. Donn√©es de test disponibles",
        "√âtapes": generate_test_steps(pdc),
        "R√©sultat attendu": generate_expected_result(pdc)
    }

def generate_test_description(pdc):
    """G√©n√©ration automatique de descriptions coh√©rentes"""
    action_map = {
        'v√©rifier': "V√©rification du bon fonctionnement de",
        'contr√¥ler': "Contr√¥le de la conformit√© de",
        'tester': "Test d'impl√©mentation de",
        'valider': "Validation du comportement de"
    }
    
    first_word = pdc.split()[0].lower()
    action = action_map.get(first_word, "Test de")
    return f"{action} {pdc[len(first_word)+1:].rstrip('.')}."

# ----------------------------
# INTERFACE STREAMLIT AM√âLIOR√âE
# ----------------------------

st.title("üìë G√©n√©rateur Automatique de Cas de Test")
tab1, tab2, tab3, tab4, tab5 = st.tabs(["üì§ Extraction", "üîç Analyse", "‚òÅÔ∏è WordCloud", "üìú R√®gles", "‚úÖ PDC & Tests"])

with tab1:
    st.header("Extraction de Texte")
    uploaded_file = st.file_uploader("T√©l√©versez un document (PDF ou DOCX)", type=["pdf", "docx"])
    
    if uploaded_file and st.button("Extraire le texte"):
        with st.spinner("Extraction en cours..."):
            extracted_text = extract_text(uploaded_file)
            
            if extracted_text:
                st.session_state.text = extracted_text
                st.success(f"Texte extrait ({len(extracted_text.split())} mots)")
                
                with st.expander("Aper√ßu du texte"):
                    st.text(extracted_text[:1500] + ("..." if len(extracted_text) > 1500 else ""))

with tab2:
    st.header("Analyse Textuelle Avanc√©e")
    
    if 'text' not in st.session_state:
        st.warning("Veuillez d'abord extraire un texte")
    else:
        nlp_model = load_nlp_model()
        if not nlp_model:
            st.error("Mod√®le NLP non disponible")
        else:
            with st.spinner("Analyse linguistique en cours..."):
                st.session_state.text_clean = clean_text(st.session_state.text, nlp_model)
                st.session_state.freq = calculate_frequencies(st.session_state.text_clean)
            
            st.subheader("Mots-cl√©s principaux")
            top_n = st.slider("Nombre de mots √† afficher", 5, 50, 15)
            st.dataframe(st.session_state.freq.head(top_n))

with tab3:
    st.header("Visualisation des Concepts")
    
    if 'text_clean' not in st.session_state:
        st.warning("Veuillez d'abord analyser un texte")
    else:
        with st.expander("Param√®tres avanc√©s"):
            col1, col2 = st.columns(2)
            with col1:
                width = st.slider("Largeur", 400, 1200, 800)
                height = st.slider("Hauteur", 200, 800, 400)
            with col2:
                bg_color = st.color_picker("Couleur de fond", "#FFFFFF")
                colormap = st.selectbox("Palette", ["viridis", "plasma", "inferno", "magma", "cividis"])
        
        if st.button("G√©n√©rer le WordCloud"):
            freq_dict = st.session_state.freq.to_dict()
            fig = generate_wordcloud(freq_dict, width, height, bg_color, colormap)
            st.pyplot(fig)

with tab4:
    st.header("Extraction des R√®gles M√©tier")
    
    if 'text' not in st.session_state:
        st.warning("Veuillez d'abord extraire un texte")
    else:
        nlp_model = load_nlp_model()
        sensitivity = st.slider("Niveau de d√©tection", 1, 5, 3,
                              help="Augmentez pour d√©tecter plus de r√®gles (peut inclure des faux positifs)")
        
        if st.button("Extraire les r√®gles", type="primary"):
            with st.spinner("Analyse approfondie en cours..."):
                rules = extract_business_rules(st.session_state.text, nlp_model, sensitivity)
                
                if rules:
                    st.session_state.rules = rules
                    st.success(f"{len(rules)} r√®gles identifi√©es")
                    
                    # Affichage pagin√©
                    items_per_page = 5
                    total_pages = (len(rules) + items_per_page - 1) // items_per_page
                    page = st.number_input("Page", 1, total_pages, 1)
                    
                    start_idx = (page - 1) * items_per_page
                    end_idx = min(start_idx + items_per_page, len(rules))
                    
                    for i in range(start_idx, end_idx):
                        st.markdown(f"**R√®gle {i+1}**")
                        st.info(rules[i])
                    
                    # Export
                    docx_file = create_rules_document(rules)
                    st.download_button(
                        "üìÑ T√©l√©charger les r√®gles",
                        data=docx_file,
                        file_name="regles_metier.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )

with tab5:
    st.header("G√©n√©ration des Tests")
    
    if 'rules' not in st.session_state:
        st.warning("Veuillez d'abord extraire les r√®gles")
    else:
        # Section PDC
        st.subheader("Points de Contr√¥le")
        if st.button("G√©n√©rer les PDC"):
            with st.spinner("Cr√©ation des PDC..."):
                st.session_state.pdc_list = [generate_pdc_from_rule(r) for r in st.session_state.rules]
                st.success(f"{len(st.session_state.pdc_list)} PDC g√©n√©r√©s")
        
        if 'pdc_list' in st.session_state:
            st.dataframe(pd.DataFrame(st.session_state.pdc_list, columns=["Points de Contr√¥le"]).head(10))
            
            # Section Cas de Test
            st.subheader("Cas de Test")
            if st.button("G√©n√©rer les Tests"):
                with st.spinner("Construction des cas de test..."):
                    st.session_state.test_cases = [
                        create_test_case(pdc, i) 
                        for i, pdc in enumerate(st.session_state.pdc_list, 1)
                    ]
                    st.success(f"{len(st.session_state.test_cases)} cas de test cr√©√©s")
            
            if 'test_cases' in st.session_state:
                st.dataframe(pd.DataFrame(st.session_state.test_cases))
                
                # Export complet
                st.download_button(
                    "üì• Exporter tous les tests (Excel)",
                    data=pd.DataFrame(st.session_state.test_cases).to_excel(),
                    file_name="cas_de_tests.xlsx",
                    mime="application/vnd.ms-excel"
                )

# ----------------------------
# FONCTIONS COMPL√âMENTAIRES
# ----------------------------

def clean_text(text, nlp_model, min_word_length=3):
    """Nettoyage approfondi du texte"""
    if not text or not nlp_model:
        return ""
    
    text = text.lower()
    text = re.sub(r"[^\w\s√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    
    doc = nlp_model(text)
    cleaned_tokens = []
    
    for token in doc:
        if (token.is_stop or token.is_punct or 
            len(token.text) < min_word_length or
            token.pos_ in ["DET", "ADP", "CCONJ", "PRON"]):
            continue
        lemma = token.lemma_.strip()
        if lemma:
            cleaned_tokens.append(lemma)
    
    return " ".join(cleaned_tokens)

def calculate_frequencies(text):
    """Calcul des fr√©quences des mots"""
    words = [word for word in text.split() if len(word) > 2]
    return pd.Series(words).value_counts()

def generate_wordcloud(freq_dict, width=800, height=400, background_color="white", colormap="viridis"):
    """G√©n√©ration du nuage de mots"""
    fig, ax = plt.subplots(figsize=(10, 5))
    wc = WordCloud(
        width=width,
        height=height,
        background_color=background_color,
        colormap=colormap,
        max_words=100
    ).generate_from_frequencies(freq_dict)
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    return fig

def create_rules_document(rules):
    """Cr√©ation d'un document Word des r√®gles"""
    doc = Document()
    doc.add_heading('R√®gles de Gestion Identifi√©es', level=1)
    for i, rule in enumerate(rules, 1):
        doc.add_paragraph(f"{i}. {rule}", style='ListBullet')
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

def generate_test_steps(pdc):
    """G√©n√©ration automatique des √©tapes de test"""
    action = pdc.split()[0].lower()
    target = ' '.join(pdc.split()[1:]).rstrip('.')
    
    steps = [
        f"1. Pr√©parer l'environnement de test",
        f"2. Ex√©cuter l'action: {action} {target}",
        f"3. Enregistrer les r√©sultats observ√©s"
    ]
    return '\n'.join(steps)

def generate_expected_result(pdc):
    """G√©n√©ration du r√©sultat attendu"""
    return f"La condition '{pdc.rstrip('.')}' est correctement respect√©e."

# ----------------------------
# PIED DE PAGE
# ----------------------------
st.markdown("---")
st.caption("¬© Outil de g√©n√©ration de tests - Version 2025")
